<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d72a1d9e9ad1a7cc8d40d05d546b5ca0",
  "translation_date": "2025-09-30T13:55:10+00:00",
  "source_file": "11-MCPServerHandsOnLabs/01-Architecture/README.md",
  "language_code": "ko"
}
-->
# í•µì‹¬ ì•„í‚¤í…ì²˜ ê°œë…

## ğŸ¯ ì´ ì‹¤ìŠµì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©

ì´ ì‹¤ìŠµì€ MCP ì„œë²„ ì•„í‚¤í…ì²˜ íŒ¨í„´, ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì›ì¹™, ê·¸ë¦¬ê³  ê²¬ê³ í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•˜ëŠ” ê¸°ìˆ ì  ì „ëµì— ëŒ€í•œ ì‹¬ì¸µì ì¸ íƒêµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ê°œìš”

ë°ì´í„°ë² ì´ìŠ¤ í†µí•©ì„ í¬í•¨í•œ í”„ë¡œë•ì…˜ ì¤€ë¹„ MCP ì„œë²„ë¥¼ êµ¬ì¶•í•˜ë ¤ë©´ ì‹ ì¤‘í•œ ì•„í‚¤í…ì²˜ ê²°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì‹¤ìŠµì—ì„œëŠ” Zava Retail ë¶„ì„ ì†”ë£¨ì…˜ì„ ê²¬ê³ í•˜ê³  ì•ˆì „í•˜ë©° í™•ì¥ ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œ, ì„¤ê³„ íŒ¨í„´, ê¸°ìˆ ì  ê³ ë ¤ ì‚¬í•­ì„ ë¶„í•´í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.

ê° ê³„ì¸µì´ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€, íŠ¹ì • ê¸°ìˆ ì´ ì™œ ì„ íƒë˜ì—ˆëŠ”ì§€, ê·¸ë¦¬ê³  ì´ëŸ¬í•œ íŒ¨í„´ì„ ìì‹ ì˜ MCP êµ¬í˜„ì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì´í•´í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.

## í•™ìŠµ ëª©í‘œ

ì´ ì‹¤ìŠµì„ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ë¶„ì„**: ë°ì´í„°ë² ì´ìŠ¤ í†µí•© MCP ì„œë²„ì˜ ê³„ì¸µí™”ëœ ì•„í‚¤í…ì²˜ ë¶„ì„
- **ì´í•´**: ê° ì•„í‚¤í…ì²˜ êµ¬ì„± ìš”ì†Œì˜ ì—­í• ê³¼ ì±…ì„
- **ì„¤ê³„**: ë‹¤ì¤‘ í…Œë„ŒíŠ¸ MCP ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì§€ì›í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„
- **êµ¬í˜„**: ì—°ê²° í’€ë§ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ì „ëµ
- **ì ìš©**: í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œì„ ìœ„í•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¡œê¹… íŒ¨í„´
- **í‰ê°€**: ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì ‘ê·¼ ë°©ì‹ ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ í‰ê°€

## ğŸ—ï¸ MCP ì„œë²„ ì•„í‚¤í…ì²˜ ê³„ì¸µ

ìš°ë¦¬ì˜ MCP ì„œë²„ëŠ” **ê³„ì¸µí™”ëœ ì•„í‚¤í…ì²˜**ë¥¼ êµ¬í˜„í•˜ì—¬ ê´€ì‹¬ì‚¬ë¥¼ ë¶„ë¦¬í•˜ê³  ìœ ì§€ë³´ìˆ˜ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤:

### ê³„ì¸µ 1: í”„ë¡œí† ì½œ ê³„ì¸µ (FastMCP)
**ì±…ì„**: MCP í”„ë¡œí† ì½œ í†µì‹  ë° ë©”ì‹œì§€ ë¼ìš°íŒ… ì²˜ë¦¬

```python
# FastMCP server setup
from fastmcp import FastMCP

mcp = FastMCP("Zava Retail Analytics")

# Tool registration with type safety
@mcp.tool()
async def execute_sales_query(
    ctx: Context,
    postgresql_query: Annotated[str, Field(description="Well-formed PostgreSQL query")]
) -> str:
    """Execute PostgreSQL queries with Row Level Security."""
    return await query_executor.execute(postgresql_query, ctx)
```

**ì£¼ìš” ê¸°ëŠ¥**:
- **í”„ë¡œí† ì½œ ì¤€ìˆ˜**: MCP ì‚¬ì–‘ ì™„ë²½ ì§€ì›
- **íƒ€ì… ì•ˆì „ì„±**: ìš”ì²­/ì‘ë‹µ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ìœ„í•œ Pydantic ëª¨ë¸
- **ë¹„ë™ê¸° ì§€ì›**: ë†’ì€ ë™ì‹œì„±ì„ ìœ„í•œ ë¹„ì°¨ë‹¨ I/O
- **ì˜¤ë¥˜ ì²˜ë¦¬**: í‘œì¤€í™”ëœ ì˜¤ë¥˜ ì‘ë‹µ

### ê³„ì¸µ 2: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê³„ì¸µ
**ì±…ì„**: ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ êµ¬í˜„ ë° í”„ë¡œí† ì½œê³¼ ë°ì´í„° ê³„ì¸µ ê°„ ì¡°ì •

```python
class SalesAnalyticsService:
    """Business logic for retail analytics operations."""
    
    async def get_store_performance(
        self, 
        store_id: str, 
        time_period: str
    ) -> Dict[str, Any]:
        """Calculate store performance metrics."""
        
        # Validate business rules
        if not self._validate_store_access(store_id):
            raise UnauthorizedError("Access denied for store")
        
        # Coordinate data retrieval
        sales_data = await self.db_provider.get_sales_data(store_id, time_period)
        metrics = self._calculate_metrics(sales_data)
        
        return {
            "store_id": store_id,
            "period": time_period,
            "metrics": metrics,
            "insights": self._generate_insights(metrics)
        }
```

**ì£¼ìš” ê¸°ëŠ¥**:
- **ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¤€ìˆ˜**: ì €ì¥ì†Œ ì ‘ê·¼ ê²€ì¦ ë° ë°ì´í„° ë¬´ê²°ì„±
- **ì„œë¹„ìŠ¤ ì¡°ì •**: ë°ì´í„°ë² ì´ìŠ¤ì™€ AI ì„œë¹„ìŠ¤ ê°„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- **ë°ì´í„° ë³€í™˜**: ì›ì‹œ ë°ì´í„°ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜
- **ìºì‹± ì „ëµ**: ë¹ˆë²ˆí•œ ì¿¼ë¦¬ì— ëŒ€í•œ ì„±ëŠ¥ ìµœì í™”

### ê³„ì¸µ 3: ë°ì´í„° ì ‘ê·¼ ê³„ì¸µ
**ì±…ì„**: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬, ì¿¼ë¦¬ ì‹¤í–‰ ë° ë°ì´í„° ë§¤í•‘

```python
class PostgreSQLProvider:
    """Data access layer for PostgreSQL operations."""
    
    def __init__(self, connection_config: Dict[str, Any]):
        self.connection_pool: Optional[Pool] = None
        self.config = connection_config
    
    async def execute_query(
        self, 
        query: str, 
        rls_user_id: str
    ) -> List[Dict[str, Any]]:
        """Execute query with RLS context."""
        
        async with self.connection_pool.acquire() as conn:
            # Set RLS context
            await conn.execute(
                "SELECT set_config('app.current_rls_user_id', $1, false)",
                rls_user_id
            )
            
            # Execute query with timeout
            try:
                rows = await asyncio.wait_for(
                    conn.fetch(query),
                    timeout=30.0
                )
                return [dict(row) for row in rows]
            except asyncio.TimeoutError:
                raise QueryTimeoutError("Query execution exceeded timeout")
```

**ì£¼ìš” ê¸°ëŠ¥**:
- **ì—°ê²° í’€ë§**: íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
- **íŠ¸ëœì­ì…˜ ê´€ë¦¬**: ACID ì¤€ìˆ˜ ë° ë¡¤ë°± ì²˜ë¦¬
- **ì¿¼ë¦¬ ìµœì í™”**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
- **RLS í†µí•©**: í–‰ ìˆ˜ì¤€ ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

### ê³„ì¸µ 4: ì¸í”„ë¼ ê³„ì¸µ
**ì±…ì„**: ë¡œê¹…, ëª¨ë‹ˆí„°ë§, êµ¬ì„±ê³¼ ê°™ì€ íš¡ë‹¨ ê´€ì‹¬ì‚¬ ì²˜ë¦¬

```python
class InfrastructureManager:
    """Infrastructure concerns management."""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.metrics = self._setup_metrics()
        self.config = self._load_configuration()
    
    def _setup_logging(self) -> Logger:
        """Configure structured logging."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('mcp_server.log')
            ]
        )
        return logging.getLogger(__name__)
    
    async def track_query_execution(
        self, 
        query_type: str, 
        duration: float, 
        success: bool
    ):
        """Track query performance metrics."""
        self.metrics.counter('query_total').labels(
            type=query_type,
            status='success' if success else 'error'
        ).inc()
        
        self.metrics.histogram('query_duration').labels(
            type=query_type
        ).observe(duration)
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ íŒ¨í„´

ìš°ë¦¬ì˜ PostgreSQL ìŠ¤í‚¤ë§ˆëŠ” ë‹¤ì¤‘ í…Œë„ŒíŠ¸ MCP ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ëª‡ ê°€ì§€ ì£¼ìš” íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤:

### 1. ë‹¤ì¤‘ í…Œë„ŒíŠ¸ ìŠ¤í‚¤ë§ˆ ì„¤ê³„

```sql
-- Core retail entities with store-based partitioning
CREATE TABLE retail.stores (
    store_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    location VARCHAR(200) NOT NULL,
    manager_id UUID NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE retail.customers (
    customer_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    store_id UUID REFERENCES retail.stores(store_id),
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE retail.orders (
    order_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    customer_id UUID REFERENCES retail.customers(customer_id),
    store_id UUID REFERENCES retail.stores(store_id),
    order_date TIMESTAMP DEFAULT NOW(),
    total_amount DECIMAL(10,2) NOT NULL,
    status VARCHAR(20) DEFAULT 'pending'
);
```

**ì„¤ê³„ ì›ì¹™**:
- **ì™¸ë˜ í‚¤ ì¼ê´€ì„±**: í…Œì´ë¸” ê°„ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
- **ì €ì¥ì†Œ ID ì „íŒŒ**: ëª¨ë“  íŠ¸ëœì­ì…˜ í…Œì´ë¸”ì— store_id í¬í•¨
- **UUID ê¸°ë³¸ í‚¤**: ë¶„ì‚° ì‹œìŠ¤í…œì„ ìœ„í•œ ì „ì—­ì ìœ¼ë¡œ ê³ ìœ í•œ ì‹ë³„ì
- **íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì **: ëª¨ë“  ë°ì´í„° ë³€ê²½ì— ëŒ€í•œ ê°ì‚¬ ê¸°ë¡

### 2. í–‰ ìˆ˜ì¤€ ë³´ì•ˆ êµ¬í˜„

```sql
-- Enable RLS on multi-tenant tables
ALTER TABLE retail.customers ENABLE ROW LEVEL SECURITY;
ALTER TABLE retail.orders ENABLE ROW LEVEL SECURITY;
ALTER TABLE retail.order_items ENABLE ROW LEVEL SECURITY;

-- Store manager can only see their store's data
CREATE POLICY store_manager_customers ON retail.customers
    FOR ALL TO store_managers
    USING (store_id = get_current_user_store());

CREATE POLICY store_manager_orders ON retail.orders
    FOR ALL TO store_managers
    USING (store_id = get_current_user_store());

-- Regional managers see multiple stores
CREATE POLICY regional_manager_orders ON retail.orders
    FOR ALL TO regional_managers
    USING (store_id = ANY(get_user_store_list()));

-- Support function for RLS context
CREATE OR REPLACE FUNCTION get_current_user_store()
RETURNS UUID AS $$
BEGIN
    RETURN current_setting('app.current_rls_user_id')::UUID;
EXCEPTION WHEN OTHERS THEN
    RETURN '00000000-0000-0000-0000-000000000000'::UUID;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

**RLSì˜ ì´ì **:
- **ìë™ í•„í„°ë§**: ë°ì´í„°ë² ì´ìŠ¤ê°€ ë°ì´í„° ê²©ë¦¬ë¥¼ ê°•ì œ
- **ì• í”Œë¦¬ì¼€ì´ì…˜ ë‹¨ìˆœí™”**: ë³µì¡í•œ WHERE ì ˆ ë¶ˆí•„ìš”
- **ê¸°ë³¸ ë³´ì•ˆ**: ì˜ëª»ëœ ë°ì´í„° ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥
- **ê°ì‚¬ ì¤€ìˆ˜**: ëª…í™•í•œ ë°ì´í„° ì ‘ê·¼ ê²½ê³„

### 3. ë²¡í„° ê²€ìƒ‰ ìŠ¤í‚¤ë§ˆ

```sql
-- Product embeddings for semantic search
CREATE TABLE retail.product_description_embeddings (
    product_id UUID PRIMARY KEY REFERENCES retail.products(product_id),
    description_embedding vector(1536),
    last_updated TIMESTAMP DEFAULT NOW()
);

-- Optimize vector similarity search
CREATE INDEX idx_product_embeddings_vector 
ON retail.product_description_embeddings 
USING ivfflat (description_embedding vector_cosine_ops);

-- Semantic search function
CREATE OR REPLACE FUNCTION search_products_by_description(
    query_embedding vector(1536),
    similarity_threshold FLOAT DEFAULT 0.7,
    max_results INTEGER DEFAULT 20
)
RETURNS TABLE(
    product_id UUID,
    name VARCHAR,
    description TEXT,
    similarity_score FLOAT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        p.product_id,
        p.name,
        p.description,
        (1 - (pde.description_embedding <=> query_embedding)) AS similarity_score
    FROM retail.products p
    JOIN retail.product_description_embeddings pde ON p.product_id = pde.product_id
    WHERE (pde.description_embedding <=> query_embedding) <= (1 - similarity_threshold)
    ORDER BY similarity_score DESC
    LIMIT max_results;
END;
$$ LANGUAGE plpgsql;
```

## ğŸ”Œ ì—°ê²° ê´€ë¦¬ íŒ¨í„´

íš¨ìœ¨ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ëŠ” MCP ì„œë²„ ì„±ëŠ¥ì— ì¤‘ìš”í•©ë‹ˆë‹¤:

### ì—°ê²° í’€ êµ¬ì„±

```python
class ConnectionPoolManager:
    """Manages PostgreSQL connection pools."""
    
    async def create_pool(self) -> Pool:
        """Create optimized connection pool."""
        return await asyncpg.create_pool(
            host=self.config.db_host,
            port=self.config.db_port,
            database=self.config.db_name,
            user=self.config.db_user,
            password=self.config.db_password,
            
            # Pool configuration
            min_size=2,          # Minimum connections
            max_size=10,         # Maximum connections
            max_inactive_connection_lifetime=300,  # 5 minutes
            
            # Query configuration
            command_timeout=30,   # Query timeout
            server_settings={
                "application_name": "zava-mcp-server",
                "jit": "off",          # Disable JIT for stability
                "work_mem": "4MB",     # Limit work memory
                "statement_timeout": "30s"
            }
        )
    
    async def execute_with_retry(
        self, 
        query: str, 
        params: Tuple = None,
        max_retries: int = 3
    ) -> List[Dict[str, Any]]:
        """Execute query with automatic retry logic."""
        
        for attempt in range(max_retries):
            try:
                async with self.pool.acquire() as conn:
                    if params:
                        rows = await conn.fetch(query, *params)
                    else:
                        rows = await conn.fetch(query)
                    return [dict(row) for row in rows]
                    
            except (ConnectionError, InterfaceError) as e:
                if attempt == max_retries - 1:
                    raise
                
                # Exponential backoff
                await asyncio.sleep(2 ** attempt)
                logger.warning(f"Database connection failed, retrying ({attempt + 1}/{max_retries})")
```

### ë¦¬ì†ŒìŠ¤ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬

```python
class MCPServerManager:
    """Manages MCP server lifecycle and resources."""
    
    async def startup(self):
        """Initialize server resources."""
        # Create database connection pool
        self.db_pool = await self.pool_manager.create_pool()
        
        # Initialize AI services
        self.ai_client = await self.create_ai_client()
        
        # Setup monitoring
        self.metrics_collector = MetricsCollector()
        
        logger.info("MCP server startup complete")
    
    async def shutdown(self):
        """Cleanup server resources."""
        try:
            # Close database connections
            if self.db_pool:
                await self.db_pool.close()
            
            # Cleanup AI client
            if self.ai_client:
                await self.ai_client.close()
            
            # Flush metrics
            await self.metrics_collector.flush()
            
            logger.info("MCP server shutdown complete")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")
    
    async def health_check(self) -> Dict[str, str]:
        """Verify server health status."""
        status = {}
        
        # Check database connection
        try:
            async with self.db_pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            status["database"] = "healthy"
        except Exception as e:
            status["database"] = f"unhealthy: {e}"
        
        # Check AI service
        try:
            await self.ai_client.health_check()
            status["ai_service"] = "healthy"
        except Exception as e:
            status["ai_service"] = f"unhealthy: {e}"
        
        return status
```

## ğŸ›¡ï¸ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µì›ë ¥ íŒ¨í„´

ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬ëŠ” MCP ì„œë²„ì˜ ì•ˆì •ì ì¸ ìš´ì˜ì„ ë³´ì¥í•©ë‹ˆë‹¤:

### ê³„ì¸µì  ì˜¤ë¥˜ ìœ í˜•

```python
class MCPError(Exception):
    """Base MCP server error."""
    def __init__(self, message: str, error_code: str = "MCP_ERROR"):
        self.message = message
        self.error_code = error_code
        super().__init__(message)

class DatabaseError(MCPError):
    """Database operation errors."""
    def __init__(self, message: str, query: str = None):
        super().__init__(message, "DATABASE_ERROR")
        self.query = query

class AuthorizationError(MCPError):
    """Access control errors."""
    def __init__(self, message: str, user_id: str = None):
        super().__init__(message, "AUTHORIZATION_ERROR")
        self.user_id = user_id

class QueryTimeoutError(DatabaseError):
    """Query execution timeout."""
    def __init__(self, query: str):
        super().__init__(f"Query timeout: {query[:100]}...", query)
        self.error_code = "QUERY_TIMEOUT"

class ValidationError(MCPError):
    """Input validation errors."""
    def __init__(self, field: str, value: Any, constraint: str):
        message = f"Validation failed for {field}: {constraint}"
        super().__init__(message, "VALIDATION_ERROR")
        self.field = field
        self.value = value
```

### ì˜¤ë¥˜ ì²˜ë¦¬ ë¯¸ë“¤ì›¨ì–´

```python
@contextmanager
async def error_handling_context(operation_name: str, user_id: str = None):
    """Centralized error handling for operations."""
    start_time = time.time()
    
    try:
        yield
        
        # Success metrics
        duration = time.time() - start_time
        metrics.operation_success.labels(operation=operation_name).inc()
        metrics.operation_duration.labels(operation=operation_name).observe(duration)
        
    except ValidationError as e:
        logger.warning(f"Validation error in {operation_name}: {e.message}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "validation",
            "field": e.field
        })
        metrics.operation_error.labels(operation=operation_name, type="validation").inc()
        raise
        
    except AuthorizationError as e:
        logger.warning(f"Authorization error in {operation_name}: {e.message}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "authorization"
        })
        metrics.operation_error.labels(operation=operation_name, type="authorization").inc()
        raise
        
    except DatabaseError as e:
        logger.error(f"Database error in {operation_name}: {e.message}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "database",
            "query": e.query[:100] if e.query else None
        })
        metrics.operation_error.labels(operation=operation_name, type="database").inc()
        raise
        
    except Exception as e:
        logger.error(f"Unexpected error in {operation_name}: {str(e)}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "unexpected"
        }, exc_info=True)
        metrics.operation_error.labels(operation=operation_name, type="unexpected").inc()
        raise MCPError(f"Internal server error in {operation_name}")
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” ì „ëµ

### ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
class QueryPerformanceMonitor:
    """Monitor and optimize query performance."""
    
    def __init__(self):
        self.slow_query_threshold = 1.0  # seconds
        self.query_stats = defaultdict(list)
    
    @contextmanager
    async def monitor_query(self, query: str, operation_type: str = "unknown"):
        """Monitor query execution time and performance."""
        start_time = time.time()
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        
        try:
            yield
            
            duration = time.time() - start_time
            
            # Record performance metrics
            self.query_stats[operation_type].append(duration)
            
            # Log slow queries
            if duration > self.slow_query_threshold:
                logger.warning(f"Slow query detected", extra={
                    "query_hash": query_hash,
                    "duration": duration,
                    "operation_type": operation_type,
                    "query": query[:200]
                })
            
            # Update metrics
            metrics.query_duration.labels(type=operation_type).observe(duration)
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"Query failed", extra={
                "query_hash": query_hash,
                "duration": duration,
                "operation_type": operation_type,
                "error": str(e)
            })
            raise
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Generate performance summary report."""
        summary = {}
        
        for operation_type, durations in self.query_stats.items():
            if durations:
                summary[operation_type] = {
                    "count": len(durations),
                    "avg_duration": sum(durations) / len(durations),
                    "max_duration": max(durations),
                    "min_duration": min(durations),
                    "slow_queries": len([d for d in durations if d > self.slow_query_threshold])
                }
        
        return summary
```

### ìºì‹± ì „ëµ

```python
class QueryCache:
    """Intelligent query result caching."""
    
    def __init__(self, redis_url: str = None):
        self.cache = {}  # In-memory fallback
        self.redis_client = redis.Redis.from_url(redis_url) if redis_url else None
        self.cache_ttl = 300  # 5 minutes default
    
    async def get_cached_result(
        self, 
        cache_key: str, 
        query_func: Callable,
        ttl: int = None
    ) -> Any:
        """Get result from cache or execute query."""
        ttl = ttl or self.cache_ttl
        
        # Try cache first
        cached_result = await self._get_from_cache(cache_key)
        if cached_result is not None:
            metrics.cache_hit.labels(type="query").inc()
            return cached_result
        
        # Execute query
        metrics.cache_miss.labels(type="query").inc()
        result = await query_func()
        
        # Cache result
        await self._set_in_cache(cache_key, result, ttl)
        
        return result
    
    def _generate_cache_key(self, query: str, user_context: str) -> str:
        """Generate consistent cache key."""
        key_data = f"{query}:{user_context}"
        return hashlib.sha256(key_data.encode()).hexdigest()
```

## ğŸ¯ ì£¼ìš” ìš”ì 

ì´ ì‹¤ìŠµì„ ì™„ë£Œí•œ í›„, ë‹¤ìŒì„ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

âœ… **ê³„ì¸µí™”ëœ ì•„í‚¤í…ì²˜**: MCP ì„œë²„ ì„¤ê³„ì—ì„œ ê´€ì‹¬ì‚¬ë¥¼ ë¶„ë¦¬í•˜ëŠ” ë°©ë²•  
âœ… **ë°ì´í„°ë² ì´ìŠ¤ íŒ¨í„´**: ë‹¤ì¤‘ í…Œë„ŒíŠ¸ ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ë° RLS êµ¬í˜„  
âœ… **ì—°ê²° ê´€ë¦¬**: íš¨ìœ¨ì ì¸ í’€ë§ ë° ë¦¬ì†ŒìŠ¤ ë¼ì´í”„ì‚¬ì´í´  
âœ… **ì˜¤ë¥˜ ì²˜ë¦¬**: ê³„ì¸µì  ì˜¤ë¥˜ ìœ í˜• ë° ë³µì›ë ¥ íŒ¨í„´  
âœ… **ì„±ëŠ¥ ìµœì í™”**: ëª¨ë‹ˆí„°ë§, ìºì‹± ë° ì¿¼ë¦¬ ìµœì í™”  
âœ… **í”„ë¡œë•ì…˜ ì¤€ë¹„**: ì¸í”„ë¼ ê´€ì‹¬ì‚¬ ë° ìš´ì˜ íŒ¨í„´  

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

**[Lab 02: ë³´ì•ˆ ë° ë‹¤ì¤‘ í…Œë„ŒíŠ¸](../02-Security/README.md)**ë¡œ ê³„ì† ì§„í–‰í•˜ì—¬ ë‹¤ìŒì„ ì‹¬ì¸µì ìœ¼ë¡œ íƒêµ¬í•˜ì„¸ìš”:

- í–‰ ìˆ˜ì¤€ ë³´ì•ˆ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­
- ì¸ì¦ ë° ê¶Œí•œ ë¶€ì—¬ íŒ¨í„´
- ë‹¤ì¤‘ í…Œë„ŒíŠ¸ ë°ì´í„° ê²©ë¦¬ ì „ëµ
- ë³´ì•ˆ ê°ì‚¬ ë° ì¤€ìˆ˜ ê³ ë ¤ ì‚¬í•­

## ğŸ“š ì¶”ê°€ ìë£Œ

### ì•„í‚¤í…ì²˜ íŒ¨í„´
- [Pythonì—ì„œì˜ í´ë¦° ì•„í‚¤í…ì²˜](https://github.com/cosmic-python/code) - Python ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ì•„í‚¤í…ì²˜ íŒ¨í„´
- [ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ íŒ¨í„´](https://en.wikipedia.org/wiki/Database_design) - ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì›ì¹™
- [ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ íŒ¨í„´](https://microservices.io/patterns/) - ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ íŒ¨í„´

### PostgreSQL ê³ ê¸‰ ì£¼ì œ
- [PostgreSQL ì„±ëŠ¥ íŠœë‹](https://wiki.postgresql.org/wiki/Performance_Optimization) - ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ê°€ì´ë“œ
- [ì—°ê²° í’€ë§ ëª¨ë²” ì‚¬ë¡€](https://www.postgresql.org/docs/current/runtime-config-connection.html) - ì—°ê²° ê´€ë¦¬
- [ì¿¼ë¦¬ ê³„íš ë° ìµœì í™”](https://www.postgresql.org/docs/current/planner-optimizer.html) - ì¿¼ë¦¬ ì„±ëŠ¥

### Python ë¹„ë™ê¸° íŒ¨í„´
- [AsyncIO ëª¨ë²” ì‚¬ë¡€](https://docs.python.org/3/library/asyncio.html) - ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° íŒ¨í„´
- [FastAPI ì•„í‚¤í…ì²˜](https://fastapi.tiangolo.com/advanced/) - í˜„ëŒ€ì ì¸ Python ì›¹ ì•„í‚¤í…ì²˜
- [Pydantic ëª¨ë¸](https://pydantic-docs.helpmanual.io/) - ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì§ë ¬í™”

---

**ë‹¤ìŒ**: ë³´ì•ˆ íŒ¨í„´ì„ íƒêµ¬í•  ì¤€ë¹„ê°€ ë˜ì…¨ë‚˜ìš”? [Lab 02: ë³´ì•ˆ ë° ë‹¤ì¤‘ í…Œë„ŒíŠ¸](../02-Security/README.md)ë¡œ ê³„ì† ì§„í–‰í•˜ì„¸ìš”.

---

**ë©´ì±… ì¡°í•­**:  
ì´ ë¬¸ì„œëŠ” AI ë²ˆì—­ ì„œë¹„ìŠ¤ [Co-op Translator](https://github.com/Azure/co-op-translator)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆìœ¼ë‚˜, ìë™ ë²ˆì—­ì—ëŠ” ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•ì„±ì´ í¬í•¨ë  ìˆ˜ ìˆìŒì„ ìœ ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ì›ë³¸ ë¬¸ì„œì˜ ì›ì–´ ë²„ì „ì´ ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¡œ ê°„ì£¼ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ì˜ ê²½ìš°, ì „ë¬¸ì ì¸ ì¸ê°„ ë²ˆì—­ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ ë²ˆì—­ ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤í•´ë‚˜ ì˜ëª»ëœ í•´ì„ì— ëŒ€í•´ ë‹¹ì‚¬ëŠ” ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.